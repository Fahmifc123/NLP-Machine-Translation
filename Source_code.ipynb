{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tugas 1 NLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sMsp4dkApHQ",
        "colab_type": "text"
      },
      "source": [
        "# Membangun Model **Bigram**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iISeDWaZmycS",
        "colab_type": "text"
      },
      "source": [
        "**Input Kalimat**\n",
        "\n",
        "Masukkan Kalimat dalam Bahasa Inggris"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE2qK11CEFW5",
        "colab_type": "code",
        "outputId": "efff7756-03de-4454-fc65-b34c5ac470c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "kalimat_terjemah = input(\"kalimat yang ingin diterjemah = \")\n",
        "kalimat_terjemah = kalimat_terjemah.lower()"
      ],
      "execution_count": 312,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kalimat yang ingin diterjemah = one\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frHmvCfLNaZE",
        "colab_type": "text"
      },
      "source": [
        "**Reading file txt**\n",
        "\n",
        "Dilakukan *read* File dari 4 artikel yang berbeda untuk kemudia di-*assign* ke variabel \"artikel\". pada proses ini 4 artikel ditampunng dalam satu variabel\n",
        "\n",
        "file .txt yang dimasukkan harus di-*upload* terlebih dahulu di google drive\n",
        "\n",
        "saat pertama kali di-jalankan *un-comment* baris kode :\n",
        "\n",
        "\n",
        "```\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "```\n",
        "\n",
        "semua harus huruf kecil pada setiap kata tidak boleh menggunakan huruf besar. Sehingga input dengan huruf besar di generate dengan fungsi string lower()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqalWZV6_eUZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 881
        },
        "outputId": "521702e2-ee3d-4295-fa6a-f228fe8a22d1"
      },
      "source": [
        "import nltk\n",
        "#di-run saat pertama kali dijalankan saja\n",
        "\n",
        "nltk.download(\"popular\")\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "line_list = []\n",
        "file_name =[]\n",
        "f1 = \"/content/drive/My Drive/NLP/PANL-BPPT-ECO-ID-150Kw.txt\"\n",
        "f2 = \"/content/drive/My Drive/NLP/PANL-BPPT-INT-ID-150Kw.txt\"\n",
        "f3 = \"/content/drive/My Drive/NLP/PANL-BPPT-SCI-ID-100Kw.txt\"\n",
        "f4 = \"/content/drive/My Drive/NLP/PANL-BPPT-SPO-ID-100Kw.txt\"\n",
        "\n",
        "file_name = [f1,f2,f3,f4]\n",
        "for i in range(3):\n",
        "  with open(file_name[i]) as f:\n",
        "    for line in f:\n",
        "      line_list.append(line)\n",
        "# assign nilai ke variabel artikel\n",
        "\n",
        "artikel=\"\"\n",
        "for word in line_list:\n",
        "  artikel += word.lower()\n",
        "\n",
        "#print(artikel)\n"
      ],
      "execution_count": 313,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeZfww0LJ9mC",
        "colab_type": "text"
      },
      "source": [
        "**Perhitungan Probabilitas**\n",
        "\n",
        "Langkah yang dilakukan untuk menghitung probabilitas Bigram :\n",
        "\n",
        "\n",
        "1.   Menghitung total kemunculan pola 2 kata\n",
        "2.   Total pada no 1 dibagi dengan total kemunculan kata pertama untuk pola tersebut\n",
        "3.   Nilai dan Pola kata disimpan dalam list\n",
        "4. ketika menemukan pola kata, maka akan ditambahkan ke dalam list(dict). Untuk kasus pola kata yang telah ada. nilai akan ditambah satu (+1)\n",
        "\n",
        "untuk catatan :\n",
        "\n",
        "\n",
        "*   Program ini tidak menampung semua jenis kemunculan pola 2 kata \n",
        "*   Program hanya akan mengambil  data yang terdapat pada artikel\n",
        "*   Secara default pola kata yang tidak dibangun akan bernilai 0 ( karena tidak di-*assign* nilai )\n",
        "*   Nilai probabilitas untuk Unigram tidak dihitung\n",
        "*   Total nilai kemunculan 1 kata dipakai untuk menghitung probabilitas untuk bigram\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7W256mLEly-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# total kemunculan kata\n",
        "freq_tab_bigram = {} \n",
        "freq_tab_unigram = {} \n",
        "\n",
        "# daftar kemunculan kata\n",
        "list_bigram = []\n",
        "list_unigram = []\n",
        "\n",
        "# nilai hasil probabilitas untuk bigram\n",
        "prob_bigram = {}\n",
        "\n",
        "# tokenisasi / split untuk artikel\n",
        "\n",
        "token = nltk.word_tokenize(artikel)\n",
        "\n",
        "n = len(token)\n",
        "\n",
        "for i in range(0,n):\n",
        "  if i < n-1 :\n",
        "    \n",
        "    list_bigram.append((token[i], token[i + 1]))\n",
        "    \n",
        "    # tabel bigram\n",
        "    \n",
        "    if (token[i],token[i+1]) in freq_tab_bigram:\n",
        "      freq_tab_bigram[(token[i],token[i+1])] += 1\n",
        "    else:\n",
        "      freq_tab_bigram[(token[i],token[i+1])] = 1\n",
        "      \n",
        "    # tabel unigram\n",
        "    \n",
        "  if token[i] in freq_tab_unigram:\n",
        "    freq_tab_unigram[token[i]] += 1\n",
        "  else:\n",
        "    freq_tab_unigram[token[i]] = 1\n",
        "    list_unigram.append(token[i])\n",
        "\n",
        "    # probabilitas bigram\n",
        "    \n",
        "for token in list_bigram:\n",
        "  if token not in prob_bigram:\n",
        "    word = token[0]\n",
        "    prob_bigram[token] = (freq_tab_bigram.get(token))/(freq_tab_unigram.get(word))\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow8XhkuCEHA4",
        "colab_type": "text"
      },
      "source": [
        "# Proses Terjemah\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmY5_oUlnCii",
        "colab_type": "text"
      },
      "source": [
        "**Mencari Terjemahan Kata**\n",
        "\n",
        "Langkah yang harus dilakukan :\n",
        "\n",
        "\n",
        "\n",
        "1.   *read* file kamus .txt yang sudah tersedia di google drive ( cara sama seperti di atas )\n",
        "2.   Dilakukan  pengecekan terhadap setiap kalimat yang ingin diterjemahkan, jika ada akan dimasukkan ke dalam list, jika tidak maka akan keluar pesan \n",
        "3. Masing - masing kata yang ingin di-terjemahkan akan dicari pada kamus yang telah disediakan dengan melihat kata pertama untuk setiap barisnya. Baris kata yang ditemukan disimpan dalam list\n",
        "4. Men- *generate* baris yang didapat dari no 3 dengan melihat kata setelahnya, kondisi dapat berbeda. metode *split* di-bawah dilakukan dengan melihat pola pada kamus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JqPg6SID_Q7",
        "colab_type": "code",
        "outputId": "5ffee95b-2566-4cb4-e34a-5ec6a0fd8e29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import re\n",
        "\n",
        "# Langkah 1\n",
        "\n",
        "line_list = []\n",
        "file_name_kamus = '/content/drive/My Drive/NLP/english_indonesia.txt'\n",
        "\n",
        "#tokenisasi untuk kalimat yang ingin diterjemahkan\n",
        "\n",
        "kata_terjemah = nltk.word_tokenize(kalimat_terjemah)\n",
        "print(kata_terjemah)\n",
        "terjemah=[]\n",
        "word_line=[]\n",
        "n = len(kata_terjemah)\n",
        "print(n)\n",
        "\n",
        "# Langkah 2\n",
        "\n",
        "for i in range(n) :\n",
        "  found = False\n",
        "\n",
        "# Langkah 3\n",
        "\n",
        "  with open(file_name_kamus) as f:\n",
        "    for line in f:\n",
        "      line_list.append(line)\n",
        "      x = re.search(\"^\"+kata_terjemah[i]+\"\\t\", line)\n",
        "      if x :\n",
        "        found = True\n",
        "        break\n",
        "    if found :\n",
        "      word_line.append(line)\n",
        "    else :\n",
        "      print(\"kata '\"+kata_terjemah[i]+\"' tidak ditemukan\")\n",
        "\n",
        "# Langkah 4\n",
        "\n",
        "if (len(word_line) < n):\n",
        "  print(\"\\ntidak bisa dilanjutkan\")\n",
        "else :\n",
        "  for i in range(n):\n",
        "    split_1 = re.split(\"\\t\",word_line[i])[1]\n",
        "    split_2 = re.split(\"\\s\",split_1)\n",
        "    split_3 = split_2[1]\n",
        "\n",
        "    #if re.findall(\"1\",split_3):\n",
        "    if \"1\" in split_2 : \n",
        "      x = split_2.index(\"1\")\n",
        "      split_3 = split_2[x+1]\n",
        "    terjemah.append(re.sub(\"[, .]\",\"\",split_3))    \n",
        "  print(terjemah)"
      ],
      "execution_count": 315,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['one']\n",
            "1\n",
            "['satu']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC693KpXUU0o",
        "colab_type": "text"
      },
      "source": [
        "**Hasil Terjamahan Kata Pada Kamus**\n",
        "\n",
        "\n",
        "Hasil terjemahan di bawah belum melewati perhitungan dengan melihat model yang telah dibangun"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12G9NgG3AoRe",
        "colab_type": "code",
        "outputId": "651bc87e-6652-4568-9105-c5f92207db6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "kata = \"\"\n",
        "for word in terjemah:\n",
        "  kata += word +\" \"\n",
        "print(kalimat_terjemah+\" = \"+kata)\n"
      ],
      "execution_count": 316,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "one = satu \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmJTXd8VZrXz",
        "colab_type": "text"
      },
      "source": [
        "# Proses Generate ke Model Bigram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE-c7PW0dme0",
        "colab_type": "text"
      },
      "source": [
        "**Cek ketersediaan Kata**\n",
        "\n",
        "Pengecekkan pada setiap kata yang ingin diterjemahkan dengan melihat keberadan kata pada tabel unigram. Jika tidak ditemukan akan dimunculkan pesan dan kata yang dimaksud akan ditambah ke dalam list unigram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5xqWBnJZubz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nilai \"terjemah\" di-assign ke \"word_indo\" untuk mempermudah pengerjaan program\n",
        "\n",
        "word_indo = terjemah\n",
        "\n",
        "word_list = []\n",
        "n = len(word_indo)\n",
        "\n",
        "for i in range(n):\n",
        "\n",
        "# pengecekkan untuk ketersediaan kata\n",
        "\n",
        "  if word_indo[i] not in list_unigram :\n",
        "    print(\"kata \"+ \"'\" + word_indo[i] + \"'\" + \" tidak tersedia\")\n",
        "    freq_tab_unigram[word_indo[i]] =  1\n",
        "    list_unigram.append(word_indo[i])\n",
        "\n",
        "# membuat pola kata \n",
        "\n",
        "  if i < n-1:\n",
        "    word_list.append((word_indo[i],word_indo[i+1]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9lXh7oTsqcM",
        "colab_type": "text"
      },
      "source": [
        "**Men-*generate* semua kemungkinan kalimat yang muncul**\n",
        "\n",
        "Dilakukan permutasi pada kalimat yang sudah diterjemahkan. Kumpulan dari kalimat-kalimat ini nantinya akan diuji probabilitasnya untuk kemudian dicari kalimat dengan probabilitias tertinggi.Semua kalimat yang muncul dibuat Pola Kata - nya"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnnuVwm-rDae",
        "colab_type": "code",
        "outputId": "402336e8-fb21-4ad2-d315-b59121b3a43c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from itertools import permutations\n",
        "\n",
        "# nilai \"word_indo\" di-assign ke \"tokens\" untuk mempermudah pengerjaan program \n",
        "tokens=word_indo\n",
        "n = len(tokens)\n",
        "\n",
        "word = []\n",
        "word_perm = []\n",
        "word_perm_list = []\n",
        "\n",
        "for i in range(n):\n",
        "  word.append(tokens[i])\n",
        "\n",
        "# permutasi untuk kalimat\n",
        "\n",
        "word_permutation = permutations(word)\n",
        "permutation_list = []\n",
        "\n",
        "print(\"kemungkinan kalimat yang muncul : \\n\")\n",
        "\n",
        "# Pembuatan Pola Kata untuk setiap kalimat yang mungkin\n",
        "\n",
        "for sentence in word_permutation:\n",
        "  n = len(sentence)\n",
        "  word_perm_temp=[]\n",
        "  kata_muncul = \"\"\n",
        "  for i in range(n):\n",
        "    kata_muncul += sentence[i] + \" \"\n",
        "    if i <n-1:\n",
        "      word_perm_temp.append((sentence[i],sentence[i+1]))\n",
        "\n",
        "  print(kata_muncul)\n",
        "\n",
        "  # Pola kata masing-masing kalimat dimasukkan ke dalam list\n",
        "\n",
        "  # Kemungkinan Pola Kata pada setiap kalimat\n",
        "\n",
        "  word_perm_list.append(word_perm_temp)\n",
        "  \n",
        "  # Kemungkinan Kalimat yang muncul\n",
        "  \n",
        "  permutation_list.append(sentence)\n",
        "\n",
        "\n"
      ],
      "execution_count": 318,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kemungkinan kalimat yang muncul : \n",
            "\n",
            "satu \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loqQ2uHesvoL",
        "colab_type": "text"
      },
      "source": [
        "**Perhitungan Probabilitas**\n",
        "\n",
        "Langkah yang dilakukan untuk menghitung probabilitas setiap kemuculan kalimat :\n",
        "\n",
        "1. Pengecekan terhadap ketersediaan pola kata pada masing-masing kalimat. Jika tidak ditemukan pola kata maka akan dilakukan metode \"Add-One\" Laplace.\n",
        "2. Setiap nilai pola kata yang terdapat pada tabel biagram akan dihitung berdasarkan *the chain rule*\n",
        "3. Hasil hitung probabilitas setiap kalimat disimpan dalam list\n",
        "\n",
        "Jika dilakukan metode \"Add-One\" :\n",
        "\n",
        "1. Metode \"Add-One\" akan menambah semua nilai kuantitas pada masing - masing nilai kemunculan pola kata.\n",
        "2. karena pada tahap sebelumnya kata yang tidak tersedia sudah dimasukkan pada tabel unigram. Maka jumlah dari *Vocab* sudah bertambah (Nilai V)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7rE2wK1zguL",
        "colab_type": "code",
        "outputId": "bd2d4ffd-2fa2-4e7f-ba70-6ee45efda4de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "n = len(word_perm_list)\n",
        "final_tab = []\n",
        "for i in range(n):\n",
        "  m = len(word_perm_list[i])\n",
        "  for j in range(m):\n",
        "    word = word_perm_list[i][j]\n",
        "\n",
        "    #kalau pola 2 kata tidak ditemukan\n",
        "\n",
        "    if word not in list_bigram:\n",
        "      print(word)\n",
        "      add_one = True\n",
        "      freq_tab_bigram[word] = 1\n",
        "      list_bigram.append(word)\n",
        "\n",
        "    # kalau pola 2 kata ditemukan\n",
        "  \n",
        "    else:\n",
        "      add_one = False\n",
        "\n",
        "  # metode laplace add-one jika tidak ditemukan pola 2 kata\n",
        "\n",
        "  if  add_one == True:\n",
        "    prob_bigram = {}\n",
        "\n",
        "    # update nilai (+1) untuk setiap elemen yang ada dan mengubah nilai probabilitas yang telah ada\n",
        "    \n",
        "    for token in word_perm_list[i]:\n",
        "      word = token[0]\n",
        "\n",
        "      #print((word))word_perm_2\n",
        "\n",
        "      prob_bigram[token] = (freq_tab_bigram.get(token) + 1)/(freq_tab_unigram.get(word)+len(freq_tab_unigram))\n",
        "  else:\n",
        "    prob_bigram = {}\n",
        "\n",
        "    # update nilai (+1) untuk setiap elemen yang ada dan mengubah nilai probabilitas yang telah ada\n",
        "    \n",
        "    for token in word_perm_list[i]:\n",
        "      word = token[0]\n",
        "      prob_bigram[token] = (freq_tab_bigram.get(token))/(freq_tab_unigram.get(word))\n",
        "\n",
        "  o = len(word_perm_list[i])\n",
        "  prob_word = 1.0\n",
        "  \n",
        "  for k in word_perm_list[i]:\n",
        "    prob_word = prob_word*(prob_bigram.get(k))\n",
        "  final_tab.append((permutation_list[i],prob_word))\n",
        " \n",
        "print(final_tab)"
      ],
      "execution_count": 319,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(('satu',), 1.0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhQ8ET1wTj99",
        "colab_type": "text"
      },
      "source": [
        "**Mencari Kalimat dengan Probabilitas Tertinggi**\n",
        "\n",
        "Ini merupakan proses terakhir yaitu mencari kalimat dengan nilai probabilitas tertinggi. List yang sudah dibangun tadi akan disortir berdasarkan nilai probabilitasnya. Kemudian menampilkan hasil kalimat dari urutan list pertama ( Setelah diurutkan secara descending)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqZ6mc6ATktw",
        "colab_type": "code",
        "outputId": "0e76b0e9-370d-4a1c-b58f-40c4ad705ad5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# fungsi untuk mengambil nilai kedua (probabilitas) untuk disorting\n",
        "def second(x):\n",
        "    return x[1]\n",
        "\n",
        "# proses sortir Descend\n",
        "\n",
        "final_tab.sort(key=second, reverse = True)\n",
        "\n",
        "terjemahan_kata = \"\"\n",
        "\n",
        "for word in final_tab[0][0]:\n",
        "  terjemahan_kata += word + \" \"\n",
        "print(final_tab[0])"
      ],
      "execution_count": 320,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(('satu',), 1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtfK2uxo2cQQ",
        "colab_type": "text"
      },
      "source": [
        "**Hasil Terjemahan Kata**\n",
        "\n",
        "Ini merupakan hasil terjemahan kata dengan melihat tabel bigram yang telah dibangun. Akan ditampilkan hasil terjemah beserta dengan nilai probabilitasnya\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SacUTYgmPNv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "5d6881bf-5f29-42b0-ca9c-431386088b68"
      },
      "source": [
        "print(kalimat_terjemah+\" = \" +terjemahan_kata)\n",
        "print(\"Probabilitas :\", final_tab[0][1])"
      ],
      "execution_count": 321,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "one = satu \n",
            "Probabilitas : 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}